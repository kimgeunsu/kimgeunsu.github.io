---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---
   
   
#### 짧은 시간동안 개략적인 내용파악을 돕기 위한 글이며 영어자료의 번역 그 자체입니다.
#### 자세한 내용은 References를 보고 시간을 투자하면 알 수 있습니다.
   
## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)   
_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._      
   
누적 보상의 최대값을 얻기 위해 가까운 시점의 보상을 희생하기도 한다.
   
   
## RL Alogrithm taxonomy
![screenshot](https://user-images.githubusercontent.com/67356449/90086469-7306b700-dd55-11ea-8524-aa5a43e38c32.png)   
_fig2. Model-Free Methods에 DQN과 Actor-Critic등의 알고리즘이 있다._   
   
   
## Key Concepts
- RL, Environment, Model, Action, State, Reward
- Markov Decision Process
- Bellman Equation   
- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
- Monte-Carlo Methods
- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network
- Policy Gradient
    - Policy Gradient Theorem
    - REINFORCE
    - Actor-Critic
    - A3C
***
   
   
### 1. RL, Environment, Model, Action, State, Reward, Policy
강화학습이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법보다
상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   

**Agent**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고
오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는
행동을 찾아내야만 한다.   

강화학습 **Agent**와 그를 둘러싼 **Environment**은 연속적인 time step속에서
서로 상호작용 한다.   

**Action**은 에이전트가 선택한 것이고, **State**는 그 선택을 하기
위한 근간이 되며, **보상**은 선택을 평가하는 기준이 된다.   
   
**Policy**는 주어진 상태에서 가장 좋은 보상을 가져다 줄 것으로 예측되는
행동을 선택하도록 한다.

**강화학습의 목적은 optimal reward를 얻기 위해 agent에게
optimal behavior strategy 즉 optimal policy를 찾는 데에 있다.**
    
    
### 2. MDP

### 3. Bellman Equation

### 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을
계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.
   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에
강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.  
   
일단 최적 가치 함수 v* 또는 q* 를 구하고 나면, 최적 정책은 쉽게 구할 수 있다.   
   
   
### 5. Monte-Carlo Methods

### 6. Temporal-Difference Learning   
**Bootstrapping**: 현재 value function 계산하는 데에 주변의
추정된 value function을 하나 이상 사용하는 것   
   
**Sarsa**: On-Policy TD control
   (TD Q + epsilon greedy policy improvement + Update every time-step)   
   
**Q-Learning**: sample-based Q-value iteration or Off-policy TD control   
importance sampling이 필요 하지 않음   
Amazing result: Q-learning은 suboptimally하게 행동 하더라도 optimal policy로 수렴한다!   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90096870-89217100-dd6f-11ea-9eca-0fde5406cb70.png)   
_fig3. MC vs TD vs DP_   
   
1. MC는 episode의 끝까지 간 후에 학습한다.
2. TD는 episode의 도중에 학습 할 수 있다.
3. DP는 다음 time step의 모든 state-action pair를 구하려고 한다.
   
   
### On-policy vs Off-policy
On-policy: "Learn on the job."   
behavior policy와 target policy가 같다.   
직접 행동 해야만 정책을 발전 시킬 수 있다.   
   
Off-policy: "Look over someone's shoulder."   
behaviour policy를 따르는 와중에 optimal value function을   
계산하기 위해 target policy를 평가한다.   
직접 행동하지 않고도 정책을 발전 시킬 수 있다.   
   
보통 Off-policy방법이 유용하며 그 이유는 다음과 같다.   
1.Off-policy방법은 full trajectory가 필요하지 않기 때문에, sample efficiency 측면에서
지난 episode에서 trajectory를 뽑아 재사용 할 수 있다. 이것이 **experience replay**이다.

2. Sample trajectory는 우리가 구하고자 하는 target policy와는 다른 behavior policy로 부터
구할 수 있는데, 이는 exploration 측면에서 더 좋을 수 있다. (더 다양한 경로를 탐색 한다는 것이다.)   
   
   
### 7. Policy Gradient
여기까지 제시된 모든 방법은 action-value method 였다.   
이 방법들은 Action의 Value(가치)를 학습하고 나서 Action value의 추정값을 기반으로 행동을 선택한다.   
   
policy gradient는 value function을 학습하는 대신 policy를 직접적으로 모델링하고   
최적화 하는 것에 주력한다.   
   
가장 큰 return을 줄 수 있는 policy π_θ에서 parameter θ를 찾는데 있어   
policy gradient가 이끄는 방향대로 θ를 바꿀 수 있게 된다.   
(θ)
policy gradient 방법들은 성능 지표 J(θ)를 최대화 하고자 하기 때문에,   
갱신 규칙은 J에 대한 Gradient Ascent(경사도 상승)규칙을 모사한 것이다.   
   
   
**Policy Gradient Theorem**: policy gradient를 계산하는 것은 까다롭다.   
주어진 environment를 알 수 없다면 policy update중에 state distribution이 어떻게 바뀌게 될 지   
추정하는 것이 어렵기 때문에 policy gradient를 계산하기 어렵다.   
   
그러나 policy gradient theorem이 이런 문제를 해결 할 수 있다. 이를 활용하면 objective function에 대한   
식을 재구성 하고, gradient 계산도 매우 간략하게 해준다.   
   
   
**REINFORCE**:
**Actor-Critic**:
   
   
***
# 개인적인 정리
   
## DQN
**Q learning이 왜 되는건지...***
Function Approximation을 Nerual Network이용한 것   
**Experience Replay**: 모든 에피소드 스텝은 하나의 replay memory에 저장된다.   
Q-learning update 동안 sample들은 replay momory에서 무작위로 drawn되기 때문에   
하나의 sample은 여러번 쓰여질 수 있다.   
Experience replay는 observation sequnces의 연관성을 제거함으로서 data efficieny를   
증가시킬수 있고, data distribution의 변화를 부드럽게 한다.   
   
   
**Periodically Updated Target**: Q 값은 오직 주기적인 업데이트를 통해 target value   
로 opimized 해 간다.   
Q network는 모든 N steps 마다 optimization target으로서 복제되고 유지된다.
이것이 short-term oscillations문제를 해결 함으로서 훈련과정을 더욱 안정되게 한다.
   
   
Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. just consider them as input feature vectors.   

**???**
   
   
## REINFORCE
Monte-Carlo Policy Gradient   
policy parameter θ를 업데이트 하기 위해 episode samples를 사용하는 Monte-Carlo 방법을 통하여 추정된 return을 얻는다. (return = reward의 합계)   
REINFORCE는 sample gradient의 기대값이 actual gradient와 같기 때문에 작동한다.   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103495-92b1d580-dd7d-11ea-8db4-d82888229afa.png)   
_fig4. REINFORCE Policy Gradient_   
   
그러므로 rael sample의 Return Gt를 측정할 수 있고, 이를 policy gradient를 update 하는 데 사용한다.   
다시한번 쓰면, REINFORCE는 full trajectory에 의존한다. REINFORCE는 Monte-Carlo Method 이기 때문이다.
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103921-4e730500-dd7e-11ea-89ea-355712da0e67.png)   
_fig4. REINFORCE algorithm_   
   
   
현재 널리 사용되는 variation은 baseline value를 추출하는 방법이다.   
이것은 **bias unchange를 유지하면서 gradient estimation의 variance를 줄이기**위해서 이다.   
(우리는 가능하다면 언제나 이렇게 되기를 원한다.)   
   
   
## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Sutton & Barto]

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code])

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Sutton & Barto]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
