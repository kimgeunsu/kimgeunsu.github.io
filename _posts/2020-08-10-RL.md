---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---
  
**수식과 그림 넣기**
   
   
#### 짧은 시간동안 개략적인 내용파악을 돕기 위한 글이며 영어자료의 번역 그 자체입니다.
#### 자세한 내용은 References를 보고 시간을 투자하면 알 수 있습니다.

## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)
_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._   
   
누적 보상의 최대값을 얻기 위해 가까운 시점의 보상을 희생하기도 한다.
   
   
## RL Alogrithm taxonomy
![screenshot](https://user-images.githubusercontent.com/67356449/90086469-7306b700-dd55-11ea-8524-aa5a43e38c32.png)
_fig2. Model-Free에 대한 분류를 알아두면 _
   
   
## Key Concepts
- RL, Environment, Model, Action, State, Reward
- Markov Decision Process
- Bellman Equation   
- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
- Monte-Carlo Methods
- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network
- Policy Gradient
    - Policy Gradient Theorem
    - REINFORCE
    - Actor-Critic
    - A3C
***
   
   
### 1. RL, Environment, Model, Action, State, Reward, Policy
강화학습이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법보다   
상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   

**Agent**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고   
오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는   
행동을 찾아내야만 한다.   

강화학습 **Agent**와 그를 둘러싼 **Environment**은 연속적인 time step속에서   
서로 상호작용 한다.   

**Action**은 에이전트가 선택한 것이고, **State**는 그 선택을 하기   
위한 근간이 되며, **보상**은 선택을 평가하는 기준이 된다.   
   
**Policy**는 주어진 상태에서 가장 좋은 보상을 가져다 줄 것으로 예측되는   
행동을 선택하도록 한다.

**강화학습의 목적은 optimal reward를 얻기 위해 agent에게   
optimal behavior strategy 즉 optimal policy를 찾는 데에 있다.**
    
    
### 2. MDP

### 3. Bellman Equation

### 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을   
계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.   
   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에   
강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.  
   
일단 최적 가치 함수 v* 또는 q* 를 구하고 나면, 최적 정책은 쉽게 구할 수 있다.   
   
   
### 5. Monte-Carlo Methods

### 6. Temporal-Difference Learning   
**Bootstrapping**: 현재 value function 계산하는 데에 주변의   
추정된 value function을 하나 이상 사용하는 것   
   
**Sarsa**: On-Policy TD control (TD Q + epsilon greedy policy improvement + Update every time-step)   
   
**Q-Learning**: sample-based Q-value iteration or Off-policy TD control   
importance sampling이 필요 하지 않음   
Amazing result: Q-learning은 suboptimally하게 행동 하더라도 optimal policy로 수렴한다!   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90096870-89217100-dd6f-11ea-9eca-0fde5406cb70.png)
_fig3. MC vs TD vs DP _
   
1. MC는 episode의 끝까지 간 후에 학습한다.
2. TD는 episode의 도중에 학습 할 수 있다.
3. DP는 모든 state-action pair를 구하려고 한다.
   
   
### On-policy vs Off-policy
On-policy: "Learn on the job."   
behaviour policy와 target policy가 같다.   
(직접 행동 해야만 정책을 발전 시킬 수 있다.)   
   
Off-policy: "Look over someone's shoulder."   
behaviour policy를 따르는 와중에 optimal value function을   
계산하기 위해 target policy를 평가한다.   
직접 행동하지 않고도 정책을 발전 시킬 수 있다.   
   
   
### 7. Policy Gradient
여기까지 제시된 모든 방법은 action-value method 였다.   
이 방법들은 행동의 가치를 학습하고 나서 행동 가치의 추정값을 기반으로 행동을 선택한다.   
   
policy gradient는 value function을 학습하는 대신 policy를 직접적으로 모델링하고   
최적화 하는 것에 주력한다.   
   
가장 큰 return을 줄 수 있는 policy pi theta에서 parameter theta를 찾는데 있어   
policy gradient가 이끄는 방향대로 theta를 바꿀 수 있게 된다.   
policy gradient 방법들은 성능 지표 J(theta)를 최대화 하고자 하기 때문에,   
갱신 규칙은 J에 대한 Gradient Ascent(경사도 상승)규칙을 모사한 것이다.   
   
   
***
   
   
## DQN은 무엇이고 왜 되는가?
Function Approximation을 Nerual Network이용한 것   
**Experience Replay**: 모든 에피소드 스텝은 하나의 replay memory에 저장된다.   
Q-learning update 동안 sample들은 replay momory에서 무작위로 drawn되기 때문에   
하나의 sample은 여러번 쓰여질 수 있다.   
Experience replay는 observation sequnces의 연관성을 제거함으로서 data efficieny를   
증가시킬수 있고, data distribution의 변화를 부드럽게 한다.   
**Periodically Updated Target**: 
   
## REINFORCE?
Monte-Carlo Policy Gradient

theta update 하는 것, Gradient Ascent = 등산하기 
   
   
   
## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Sutton & Barto]

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code])

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Sutton & Barto]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
