---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---
  
**수식과 그림 넣기**
   
   
#### 짧은 시간동안 개략적인 내용파악을 돕기 위한 글이며 영어자료의 번역 그 자체입니다.
#### 자세한 내용은 References를 보고 시간을 투자하면 알 수 있습니다.

## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)

_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._   
   
누적 보상의 최대값을 얻기 위해 가까운 시점의 보상을 희생하기도 한다.
   
   
   
## RL Alogrithm taxonomy
![screenshot](https://user-images.githubusercontent.com/67356449/90086469-7306b700-dd55-11ea-8524-aa5a43e38c32.png)
_fig2. Model-Free에 대한 분류를 알아두면 _
   
   
## Key Concepts
- RL, Environment, Model, Action, State, Reward
   
- Markov Decision Process
   
- Bellman Equation
   
- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
   
- Monte-Carlo Methods
   
- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network
   
- Policy Gradient
    - REINFORCE
    - Actor-Critic
    - A3C
   
   
***
### 1. RL, Environment, Model, Action, State, Reward, Policy
강화학습이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법보다   
상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   

**Agent**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고   
오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는 행동을 찾아내야만 한다.   

강화학습 **Agent**와 그를 둘러싼 **Environment**은 연속적인 time step속에서 서로 상호작용 한다.   

**Action**은 에이전트가 선택한 것이고, **State**는 그 선택을 하기   
위한 근간이 되며, **보상**은 선택을 평가하는 기준이 된다.   
   
**Policy**는 주어진 상태에서 가장 좋은 보상을 가져다 줄 것으로 예측되는 행동을 선택하도록 한다.

    
### 2. MDP

### 3. Bellman Equation

### 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을   
계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에   
강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.   
일단 최적 가치 함수 v* 또는 q* 를 구하고 나면, 최적 정책은 쉽게 구할 수 있다.   
   
   
### 5. Monte-Carlo Methods

### 6. Temporal-Difference Learning
TD(0), Sarsa(lambda)   
**Bootstrapping**: 현재 value function 계산하는 데에 주변의   
추정된 value function을 하나 이상 사용하는 것   
   
   
**Sarsa**: On-Policy TD control (TD Q + epsilon greedy policy improvement + Update every time-step)   
   
**Q-Learning**: sample-based Q-value iteration or Off-policy TD control   
importance sampling이 필요 하지 않음
Q-Learning Properties   
Amazing result: Q-learning optimal policy로 수렴한다. (suboptimally하게 행동 하더라도!)   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90086562-ab0dfa00-dd55-11ea-9302-8d6c02fd7ce1.png)
_fig3. Model-Free에 대한 분류를 알아두면 _

### On-policy vs Off-policy
On-policy: "Learn on the job."   
behaviour policy와 target policy가 같다. (직접 행동 해야만 정책을 발전 시킬 수 있다.)   
   
Off-policy: "Look over someone's shoulder."   
behaviour policy를 따르는 와중에 optimal value function을 계산하기 위해 target policy를 평가함.   
직접 행동하지 않고도 정책을 발전 시킬 수 있다.   
   
   
## DQN은 무엇이고 왜 되는가?
Function Approximation을 Nerual Network이용한 것   
**Experience Replay**: 모든 에피소드 스텝은 하나의 replay memory에 저장된다. Q-learning update 동안 sample들은   
replay momory에서 무작위로 drawn되기 때문에 하나의 sample은 여러번 쓰여질 수 있음.   
Experience replay는 observation sequnces의 연관성을 제거함으로서 data efficieny를   
증가시킬수 있고, data distribution의 변화를 부드럽게 한다.   
**Periodically Updated Target**: 
   
## REINFORCE는 왜 되는가?
Monte-Carlo Policy Gradient
theta update 하는 것, Gradient Ascent = 등산하기 
   
   
   
## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Sutton & Barto]

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code])
(이 자료는 Silver's Lecture에 기반하며 Silver는 Sutton의 책을 기준으로 설명)

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Sutton & Barto]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
