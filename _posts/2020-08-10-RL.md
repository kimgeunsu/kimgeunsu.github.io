---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---   
## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)   
_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._   
   
누적 보상의 최대값을 얻기 위해 가까운 시점의 최대 보상값을 얻는 행동을 희생(선택하지 않기)하기도 한다.
   
   
## RL Alogrithm taxonomy
![screenshot](https://user-images.githubusercontent.com/67356449/90086469-7306b700-dd55-11ea-8524-aa5a43e38c32.png)   
_fig2. Model-Free Methods에 DQN과 Actor-Critic등의 알고리즘이 있다._   
   
   
## Contents
- RL, Environment, Model, Action, State, Reward
- Markov Decision Processes
- Bellman Equation   
- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
- Monte-Carlo Methods
- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network
- Policy Gradient
    - Policy Gradient Theorem
    - REINFORCE
    - Actor-Critic
    - A3C
    
***
   
   
## 1. RL, Environment, Model, Action, State, Reward, Policy
_(강화학습, 환경, 모델, 행동, 상태, 보상, 정책)_   

**Reinforcement Learning**이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법(지도학습, 비지도학습) 보다 상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   

**Agent**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고 오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는 행동을 찾아내야만 한다.   

강화학습 **Agent**와 그를 둘러싼 **Environment**은 연속적인 time step속에서 서로 상호작용 한다.   

**Action**은 에이전트가 선택한 것이고, **State**는 그 선택을 하기 위한 근간이 되며, **Reward**는 선택을 평가하는 기준이 된다.   
   
**Policy**는 특정 state에서 reward의 총합이 최대가 되게 하는 optimal action을 선택하도록 하는 guideline을 제공한다.

**강화학습의 목적은 optimal reward를 얻기 위해 agent에게 optimal policy를 찾도록 하는 것이다.**
    
    
## 2. Markov Decision Process
강화 학습은 주로 Markov decision process (MDP)라는 확률 모델로 표현된다. MDP는 **Sequential Decision Making(순차적 의사결정 과정)** 을 확률과 그래프를 이용하여 모델링한 것이며 first-order Markov assumption을 기반으로 고안 되었다.
MDP의 모든 상태들은 "Markov" 특성을 가지는데, Markov assumption이란 미래가 과거가 아닌 오직 current state에 의존한다는 것이다. 다음 **t+1** time step의 상태를 구하기 위해서는 오직 현재 **t** time step에서의 states 가 필요하지, 과거 **t-1** time step의 states가 필요하지 않다는 것이다.
   
Markov decision process는 다섯 요소로 구성된다.   
![screenshot](https://user-images.githubusercontent.com/67356449/90248993-a8ee8d00-de74-11ea-954b-104358f27bad.png)   
   
- S - states
- A - actions
- P - state transition probability
- R - reward function
- γ - discounting factor
  
  
unknown environment란 P와 R에 대한 완벽한 정보가 없는 상태를 말한다.   
discount(감가)가 왜 필요한지는 Silver 강의 lecture 2에 있다.
  
  
Sequential Decision Making and non i.i.d   
independent and identically distributed: 확률변수가 여러 개 있을 때 (X1 , X2 , ... , Xn) 이들이 상호독립적이며, 모두 동일한 확률분포 f(x)를 가진다면 iid이다. RL은 Non i.i.d data를 다룬다.
   
   
**Markov Chain**   
![screenshot](https://user-images.githubusercontent.com/67356449/90249359-4649c100-de75-11ea-9f51-3994de477c8c.png)   
_fig. state에서 state로 옮겨지는 확률이 표시된 Markov chain example_   
현재는 어떤 state에서 state로 가는 확률이 표시되어 있지만 MDP에서는 action을 할 확률과 action을 해서 어떤 state로 갈 확률이 주어지게 된다.   
   
   
## 3. Bellman Equation
     
### Silver lecture에서 가져올 것
   
   
## 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을 계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.   
   
이 때의 문제는 Optimal Substructure, Overlapping Subproblem 인 조건이여야 하며, DP라는 것은 답을 구하기 위해서 했던 계산을 또 하고 또 하고 계속해야하는 종류의 문제를 말한다.   
   
   
**Optimal Substructure**: Optimal solution은 Subproblems로 Decomposed(분해) 할 수 있다.   
**Overlapping Subproblem**: Subproblems는 Recur(반복)되며, Solutions는 Chaced(저장)되고 Reused(재사용)할 수 있다.   
   
   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에 강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.  
   
일단 Optimal Value Function v* 또는 q* 를 구하고 나면, Optimal Policy(최적 정책)은 쉽게 구할 수 있다.   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90112142-939d3400-dd8a-11ea-89ec-c458d0447586.png)   
_fig3. Policy Iteration_   
  
  
Policy Evaluation과 Improvement를 반복해가며 Optimal value function 을 찾게 된다.   
   
   
**backup diagram 넣기**   
   
   
### Prediction vs Control
**Prediction**: _Estimate_ the value function of an _unknown_ MDP   
   
**Control**: _Optimise_ the value function of an _unknown_ MDP   
   
### On-policy vs Off-policy
***On-policy***: "Learn on the job."   
behavior policy와 target policy가 같다.   
직접 행동 해야만 정책을 발전 시킬 수 있다.   
   
***Off-policy***: "Look over someone's shoulder."   
behavior policy를 따르는 와중에 optimal value function을 계산하기 위해 target policy를 평가한다.   
직접 행동하지 않고도 정책을 발전 시킬 수 있다.   
   
보통 Off-policy방법이 유용하며 그 이유는 다음과 같다.   
1. Off-policy방법은 full trajectory가 필요하지 않기 때문에, sample efficiency 측면에서 지난 episode에서 trajectory를 뽑아 재사용 할 수 있다. 이것이 **experience replay**이다.    
2. Sample trajectory는 우리가 구하고자 하는 target policy와는 다른 behavior policy로 부터 구할 수 있는데, 이는 exploration 측면에서 더 좋을 수 있다. (더 다양한 경로를 탐색 한다는 것이다.)   
   
## 5. Monte-Carlo Methods

## 6. Temporal-Difference Learning
_강화학습에 있어서 중심이 되는 새로운 개념 하나를 뽑아야 한다면 그것은 의심의 여지 없이 Temporal-Difference Learning일 것이다. - RL: An introduction - Sutton & Barto -_   
   
TD learning은 MC와 DP 방법을 합친 것이다. TD는 MC처럼 환경에 대한 모델 없이도 경험으로부터 직접 학습할 수 있다. 또한 DP와 마찬가지로 최종결과를 얻을 때 까지 기다리지 않고, 부분적으로 다른 추정값을 기반으로 추정값을 갱신한다.   
   
실제 결과와 오차간의 차이가 시간에 따른 예측값의 변화량(시간 차)에 비례한다. 그렇기 때문에 Temporal-Difference Learning 이라고 한다.
   
   
**Bootstrapping**: 현재 value function 계산하는 데에 주변의 추정된 value function을 하나 이상 사용하는 것이다. 전술했듯이, Bellman Equation 자체가 Bootstrapping이다.   
   
**Sarsa**: On-Policy TD control   
On-polcity TD learning: Sarsa(λ) -> 보상을 얻기 전 몇 time step만큼의 행동을 강화한다.
   (TD Q + epsilon greedy policy improvement + Update every time-step)   
   
**Q-Learning**: sample-based Q-value iteration or Off-policy TD control   
당신은 어떤 게임을 하는데 Q라는 사람을 옆에 두고 매번 Q의 의견을 물어서 플레이에 참고하는 것이다. 학습을 처음 시작할 때는 Q의 실력은 인간 이하이다. 강화학습을 한다는 것은 Q의 의견대로 게임을 한 뒤 졌으면 혼내고 이기면 칭찬해서 Q의 판단력을 프로게이머 수준까지 키우는 것을 말한다. 만약 Q가 프로게이머급 실력을 가지고 있다면, 당연히 최고 수준의 플레이를 할 수 있을 것이다.   
이와같이 Q-learning은 간접적인 학습 방식이다. 왜냐하면 action을 통해 간접적으로 optimal policy를 찾아가지, 직접적으로 policy를 개선하지 않기 때문이다.   
Amazing result: Q-learning은 suboptimally하게 행동 하더라도 optimal policy로 수렴한다는 것이 수학적으로 증명되어있다!   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90096870-89217100-dd6f-11ea-9eca-0fde5406cb70.png)   
_fig4. MC vs TD vs DP_   
   
   
1. MC는 episode의 끝까지 간 후에 total return **Gt**를 얻는다. 
2. TD는 time step의 행동 직후에 바로 reward를 얻는다.
3. DP는 다음 time step에 대한 모든 reward와 value function을 구한다.
   
   
- MC: High variance, Zero bias, Not bootstap, Samples
- TD: Low variance, Some bias, Bootstraps, Samples
- DP: Bootstraps, Not sample
  
  
## 7. Policy Gradient
여기까지 제시된 모든 방법은 action-value method 였다. 이 방법들은 Action의 Value(가치)를 학습하고 나서 Action value의 추정값을 기반으로 행동을 선택한다.   
   
policy gradient는 value function을 학습하는 대신 policy를 직접적으로 모델링하고 최적화 하는 것에 주력한다.   
   
가장 큰 return을 줄 수 있는 policy π_θ에서 parameter θ를 찾는데 있어 policy gradient가 이끄는 방향대로 θ를 바꿀 수 있게 된다. policy gradient 방법들은 성능 지표 J(θ)를 최대화 하고자 하기 때문에, 갱신 규칙은 J에 대한 Gradient Ascent(경사도 상승)규칙을 모사한 것이다.   
   
   
**Policy Gradient Theorem**: policy gradient를 계산하는 것은 까다롭다. 주어진 environment를 알 수 없다면 policy update중에 state distribution이 어떻게 바뀌게 될 지 추정하는 것이 어렵기 때문에 policy gradient를 계산하기 어렵게 한다.   
   
그러나 policy gradient theorem이 이런 문제를 해결 할 수 있다. 이를 활용하면 objective function에 대한 식을 재구성 하고, gradient 계산도 매우 간략하게 해준다.   
   
   
**REINFORCE**: 가장 간단한 policy optimization method
  
**Actor-Critic**: Value-based method와 Policy based method를 섞은 것.
   
   
***
   
   
# 개인적인 정리
   
   
## DQN
**DQN이 왜 되는건지...**
DQN은 Deep Reinforcement learning의 first major successfule demonstration이다.
Function Approximation을 Nerual Network이용한 것   -> function approximation이란? 정리
   
   
완벽한 Q가 존재하며 학습 가능하다는 것은 수학적으로 증명됐지만 현재까지 방법으로는 Q를 제대로 학습시키는 것이 매우 어려웠다는 것이다. 아주 단순한 게임만 학습시킬 수 있었고 조금만 복잡해도 학습이 잘 안되었는데, function approxiator를 Deep Neural Network로 구성하여 optimal policy를 제대로 학습시킬 수 있는 아이디어를 제시해 강화학습의 효율을 크게 끌어올린 것이다.   
   
   
**Experience Replay**: DQN학습을 안정화 시키는 것에 아주 중요하다.   
모든 에피소드 스텝은 하나의 replay memory에 저장된다.   
Q-learning update 동안 sample들은 replay momory에서 무작위로 drawn되기 때문에 하나의 sample은 여러번 쓰여질 수 있다.   
Experience replay는 observation sequnces의 연관성을 제거함으로서 data efficieny를 증가시킬수 있고, data distribution의 변화를 부드럽게 만든다.   
   
Experience replay는 regular intervals에서 crashing behavior를 반복하도록 한다. 이러면 entire learning process에서 trajectory를 유지할 수 있게 하는데, trajectory를 반복하는 것은 value function이 더 이상 방문하지 않을 state space의 일부에서 여전히 정확하다는 것을 보장하게 한다.   
   
   
**Periodically Updated Target**: Q 값은 오직 주기적인 업데이트를 통해 target value 로 opimized 해 간다.   
Q network는 모든 N steps 마다 optimization target으로서 복제되고 유지된다.**(????)**   
이것이 short-term oscillations문제를 해결 함으로서 훈련과정을 더욱 안정되게 한다.   
   
   
Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. just consider them as input feature vectors.   
   
   
**논문에서 캡쳐하여 가져올 것**
1. epsilon-greedy policy로 action을 택한다.
2. transition(s_t, a_t, r_t+1, s_t+1)을 replay memory **D**에 저장한다.
3. random mini-batch of transitions(s, a, r, s')을 **D**로 부터 sample한다. (sample = trajectory)
4. old, fixed parameters w- 에 대한 Q-learning target을 계산한다.
5. Q-nerwork와 Q-learning targets 사이의 Mean Square Error를 optimise 한다.
6. variant of stochastic gradient descent를 이용한다.
   
   
## REINFORCE
Monte-Carlo Policy Gradient   
policy parameter θ를 업데이트 하기 위해 episode samples를 사용하는 Monte-Carlo 방법을 통하여 추정된 return을 얻는다. (return = reward의 합계)   
REINFORCE는 sample gradient의 기대값이 actual gradient와 같기 때문에 작동한다.   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103495-92b1d580-dd7d-11ea-8db4-d82888229afa.png)   
_fig5. REINFORCE Policy Gradient_   
   
그러므로 rael sample의 Return **Gt**를 측정할 수 있고, 이를 policy gradient를 update 하는 데 사용한다.   
다시한번 쓰면, REINFORCE는 full trajectory에 의존한다. REINFORCE는 Monte-Carlo Method 이기 때문이다.
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103921-4e730500-dd7e-11ea-89ea-355712da0e67.png)   
_fig6. REINFORCE algorithm_   
   
   
현재 널리 사용되는 variation은 baseline value를 추출하는 방법이다.   
이것은 **bias unchange를 유지하면서 gradient estimation의 variance를 줄이기**위해서 이다.   
(우리는 가능하다면 언제나 이렇게 되기를 원한다.)   
   
   
## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Reinforcement Learning: An Introduction - Sutton & Barto -], [Sutton's HP]

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code])

Code: [minimal-RL] 

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Reinforcement Learning: An Introduction - Sutton & Barto -]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[Sutton's HP]: http://incompleteideas.net/
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
[minimal-RL]: https://github.com/seungeunrho/minimalRL
