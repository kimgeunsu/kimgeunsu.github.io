---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---
   
   
#### 짧은 시간동안 개략적인 내용파악을 돕기 위한 글이며 영어자료의 번역 그 자체입니다.
#### 자세한 내용은 맨 밑에 있는 References를 보고 시간을 투자하면 알 수 있습니다.
   
## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)   
_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._   
   
누적 보상의 최대값을 얻기 위해 가까운 시점의 최대 보상값을 얻는 행동을 희생(선택하지 않기)하기도 한다.
   
   
## RL Alogrithm taxonomy
![screenshot](https://user-images.githubusercontent.com/67356449/90086469-7306b700-dd55-11ea-8524-aa5a43e38c32.png)   
_fig2. Model-Free Methods에 DQN과 Actor-Critic등의 알고리즘이 있다._   
   
   
## Contents
- RL, Environment, Model, Action, State, Reward
- Markov Decision Processes
- Bellman Equation   
- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
- Monte-Carlo Methods
- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network
- Policy Gradient
    - Policy Gradient Theorem
    - REINFORCE
    - Actor-Critic
    - A3C
    
***
   
   
## 1. RL, Environment, Model, Action, State, Reward, Policy
_(강화학습, 환경, 모델, 행동, 상태, 보상, 정책)_   

**Reinforcement Learning**이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법(지도학습, 비지도학습) 보다 상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   

**Agent**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고 오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는 행동을 찾아내야만 한다.   

강화학습 **Agent**와 그를 둘러싼 **Environment**은 연속적인 time step속에서 서로 상호작용 한다.   

**Action**은 에이전트가 선택한 것이고, **State**는 그 선택을 하기 위한 근간이 되며, **Reward**는 선택을 평가하는 기준이 된다.   
   
**Policy**는 특정 state에서 reward의 총합이 최대가 되게 하는 optimal action을 선택하도록 하는 guideline을 제공한다.

**강화학습의 목적은 optimal reward를 얻기 위해 agent에게 optimal policy를 찾도록 하는 것이다.**
    
    
## 2. Markov Decision Process
모든 RL problems는 MDPs로 치환할 수 있다. MDP의 모든 상태들은 "Markov" 특성을 가지는데, 이것은 미래가 과거가 아닌 오직 current state에 의존한다는 것이다. 다음 **t+1** time step의 상태를 구하기 위해서는 오직 현재 **t** time step에서의 states 가 필요하지, 과거 **t-1** time step의 states가 필요하지 않다는 것이다.    
   
Markov decision process는 다섯 요소로 구성된다.   
![screenshot](https://user-images.githubusercontent.com/67356449/90248993-a8ee8d00-de74-11ea-954b-104358f27bad.png)   
   
- S - states
- A - actions
- P - transition probability function
- R - reward function
- γ - discounting factor for future rewards (0-1 값을 가진다.)
  
  
unknown environment란 P와 R에 대한 완벽한 정보가 없는 상태를 말한다.   
discount(감가)가 왜 필요한지는 Silver 강의 lecture 2에 있다.
  
  
**Markov Chain**   
![screenshot](https://user-images.githubusercontent.com/67356449/90249359-4649c100-de75-11ea-9f51-3994de477c8c.png)   
_fig. state에서 state로 옮겨지는 확률이 표시된 Markov chain example_   
현재는 어떤 state에서 state로 가는 확률이 표시되어 있지만 MDP에서는 action을 할 확률과 action을 해서 어떤 state로 갈 확률이 주어지게 된다.   
   
   
## 3. Bellman Equation
     
![screenshot](https://user-images.githubusercontent.com/67356449/90330317-e07d4680-dfe6-11ea-993d-870d40d25cdf.png)   
_fig. return은 미래의 시점일수록 가중치를 적게 하는 보상의 총합을 말한다._  
   
    
![screenshot](https://user-images.githubusercontent.com/67356449/90330443-ac565580-dfe7-11ea-8d27-94363af551c8.png)   
_fig. value function은 state s에 있을 때 얻을 수 있을 것으로 예상되는 보상의 총합을 말한다._   
   
### action-value function
![screenshot](https://user-images.githubusercontent.com/67356449/90330496-01926700-dfe8-11ea-8cb4-ad492a801e95.png)   
_fig. action-value function은 어떤 action를 선택하는 것에 대한 가치를 얻을 수 있다._   
   
     
![screenshot](https://user-images.githubusercontent.com/67356449/90330356-2803d280-dfe7-11ea-9054-8037f8b07ef9.png)   
_fig. Bellman equation은 현재 value function와 다음 time step의 value function의 관계를 말해준다._   
   
 
![screenshot](https://user-images.githubusercontent.com/67356449/90330522-3a324080-dfe8-11ea-81f4-415395332a50.png)   
_fig. _   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90330406-6f8a5e80-dfe7-11ea-8464-4b66d8cfae9c.png)   
_fig. _   
   
   
## 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을 계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.   
   
이 때의 문제는 Optimal Substructure, Overlapping Subproblem 인 조건이여야 하며, DP라는 것은 답을 구하기 위해서 했던 계산을 또 하고 또 하고 계속해야하는 종류의 문제를 말한다.   
   
   
**Optimal Substructure**: Optimal solution은 Subproblems로 Decomposed(분해) 할 수 있다.   
**Overlapping Subproblem**: Subproblems는 Recur(반복)되며, Solutions는 Chaced(저장)되고 Reused(재사용)할 수 있다.   
   
   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에 강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.  
   
일단 Optimal Value Function v* 또는 q* 를 구하고 나면, Optimal Policy(최적 정책)은 쉽게 구할 수 있다.   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90112142-939d3400-dd8a-11ea-89ec-c458d0447586.png)   
_fig3. Policy Iteration_   
  
  
Policy Evaluation과 Improvement를 반복해가며 Optimal value function 을 찾게 된다.   
   
   
**backup diagram 넣기**   
   
   
### Prediction vs Control
**Prediction**: _Estimate_ the value function of an _unknown_ MDP   
   
**Control**: _Optimise_ the value function of an _unknown_ MDP   
   
### On-policy vs Off-policy
***On-policy***: "Learn on the job."   
behavior policy와 target policy가 같다.   
직접 행동 해야만 정책을 발전 시킬 수 있다.   
   
***Off-policy***: "Look over someone's shoulder."   
behavior policy를 따르는 와중에 optimal value function을 계산하기 위해 target policy를 평가한다.   
직접 행동하지 않고도 정책을 발전 시킬 수 있다.   
   
보통 Off-policy방법이 유용하며 그 이유는 다음과 같다.   
1. Off-policy방법은 full trajectory가 필요하지 않기 때문에, sample efficiency 측면에서 지난 episode에서 trajectory를 뽑아 재사용 할 수 있다. 이것이 **experience replay**이다.    
2. Sample trajectory는 우리가 구하고자 하는 target policy와는 다른 behavior policy로 부터 구할 수 있는데, 이는 exploration 측면에서 더 좋을 수 있다. (더 다양한 경로를 탐색 한다는 것이다.)   
   
## 5. Monte-Carlo Methods

## 6. Temporal-Difference Learning
_강화학습에 있어서 중심이 되는 새로운 개념 하나를 뽑아야 한다면 그것은 의심의 여지 없이 Temporal-Difference Learning일 것이다. - RL: An introduction - Sutton & Barto -_   
   
TD learning은 MC와 DP 방법을 합친 것이다. TD는 MC처럼 환경에 대한 모델 없이도 경험으로부터 직접 학습할 수 있다. 또한 DP와 마찬가지로 최종결과를 얻을 때 까지 기다리지 않고, 부분적으로 다른 추정값을 기반으로 추정값을 갱신한다.   
   
실제 결과와 오차간의 차이가 시간에 따른 예측값의 변화량(시간 차)에 비례한다. 그렇기 때문에 Temporal-Difference Learning 이라고 한다.
   
   
**Bootstrapping**: 현재 value function 계산하는 데에 주변의 추정된 value function을 하나 이상 사용하는 것이다. 전술했듯이, Bellman Equation 자체가 Bootstrapping이다.   
   
**Sarsa**: On-Policy TD control   
   (TD Q + epsilon greedy policy improvement + Update every time-step)   
   
**Q-Learning**: sample-based Q-value iteration or Off-policy TD control   
당신은 축구를 하는데 아주 초보적인 상태이며 역시 초보자인 Q라는 친구에게서 충고를 들으며 축구를 한다. 처음에는 Q라는 친구 역시 어떻게 해야 축구를 잘 할 수 있는지 모르지만, 시간이 지나며 어떤 것이 많은 보상을 얻는 '좋은' 행동인지 알게된다. Q라는 친구의 축구실력이 프로에 근접하게 되어, 당신 역시 축구를 프로수준으로 잘하게 될 거라는 것이 Q-learning이다. 이와같이 Q-learning은 간접적인 학습 방식이다. 왜냐하면 action을 통해 간접적으로 optimal policy를 찾아가지, 직접적으로 policy를 개선하지 않기 때문이다.   
   
importance sampling이 필요 하지 않음   
Amazing result: Q-learning은 suboptimally하게 행동 하더라도 optimal policy로 수렴한다!   
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90096870-89217100-dd6f-11ea-9eca-0fde5406cb70.png)   
_fig4. MC vs TD vs DP_   
   
   
1. MC는 episode의 끝까지 간 후에 total return **Gt**를 얻는다. 
2. TD는 time step의 행동 직후에 바로 reward를 얻는다.
3. DP는 다음 time step에 대한 모든 reward와 value function을 구한다.
   
   
- MC: High variance, Zero bias, Not bootstap, Samples
- TD: Low variance, Some bias, Bootstraps, Samples
- DP: Bootstraps, Not sample
  
  

   
## 7. Policy Gradient
여기까지 제시된 모든 방법은 action-value method 였다. 이 방법들은 Action의 Value(가치)를 학습하고 나서 Action value의 추정값을 기반으로 행동을 선택한다.   
   
policy gradient는 value function을 학습하는 대신 policy를 직접적으로 모델링하고 최적화 하는 것에 주력한다.   
   
가장 큰 return을 줄 수 있는 policy π_θ에서 parameter θ를 찾는데 있어 policy gradient가 이끄는 방향대로 θ를 바꿀 수 있게 된다. policy gradient 방법들은 성능 지표 J(θ)를 최대화 하고자 하기 때문에, 갱신 규칙은 J에 대한 Gradient Ascent(경사도 상승)규칙을 모사한 것이다.   
   
   
**Policy Gradient Theorem**: policy gradient를 계산하는 것은 까다롭다. 주어진 environment를 알 수 없다면 policy update중에 state distribution이 어떻게 바뀌게 될 지 추정하는 것이 어렵기 때문에 policy gradient를 계산하기 어렵게 한다.   
   
그러나 policy gradient theorem이 이런 문제를 해결 할 수 있다. 이를 활용하면 objective function에 대한 식을 재구성 하고, gradient 계산도 매우 간략하게 해준다.   
   
   
**REINFORCE**:
  
**Actor-Critic**:
   
   
***
   
   
   
# 개인적인 정리
   
   
## DQN
**Q learning이 왜 되는건지...**
Function Approximation을 Nerual Network이용한 것   
**Experience Replay**: 모든 에피소드 스텝은 하나의 replay memory에 저장된다.   
Q-learning update 동안 sample들은 replay momory에서 무작위로 drawn되기 때문에 하나의 sample은 여러번 쓰여질 수 있다.   
Experience replay는 observation sequnces의 연관성을 제거함으로서 data efficieny를 증가시킬수 있고, data distribution의 변화를 부드럽게 만든다.   
   
   
**Periodically Updated Target**: Q 값은 오직 주기적인 업데이트를 통해 target value 로 opimized 해 간다.   
Q network는 모든 N steps 마다 optimization target으로서 복제되고 유지된다.***(????)***   
이것이 short-term oscillations문제를 해결 함으로서 훈련과정을 더욱 안정되게 한다.   
   
   
Algorithm for DQN with experience replay and occasionally frozen optimization target. The prepossessed sequence is the output of some processes running on the input images of Atari games. just consider them as input feature vectors.   

**???**
   
   
## REINFORCE
Monte-Carlo Policy Gradient   
policy parameter θ를 업데이트 하기 위해 episode samples를 사용하는 Monte-Carlo 방법을 통하여 추정된 return을 얻는다. (return = reward의 합계)   
REINFORCE는 sample gradient의 기대값이 actual gradient와 같기 때문에 작동한다.   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103495-92b1d580-dd7d-11ea-8db4-d82888229afa.png)   
_fig5. REINFORCE Policy Gradient_   
   
그러므로 rael sample의 Return **Gt**를 측정할 수 있고, 이를 policy gradient를 update 하는 데 사용한다.   
다시한번 쓰면, REINFORCE는 full trajectory에 의존한다. REINFORCE는 Monte-Carlo Method 이기 때문이다.
   
   
![screenshot](https://user-images.githubusercontent.com/67356449/90103921-4e730500-dd7e-11ea-89ea-355712da0e67.png)   
_fig6. REINFORCE algorithm_   
   
   
현재 널리 사용되는 variation은 baseline value를 추출하는 방법이다.   
이것은 **bias unchange를 유지하면서 gradient estimation의 variance를 줄이기**위해서 이다.   
(우리는 가능하다면 언제나 이렇게 되기를 원한다.)   
   
   
## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Reinforcement Learning: An Introduction - Sutton & Barto -], [Sutton's tutorial], [minimal-RL]  

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code]), [Sutton note], [SanghyukChun's Blog]

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Reinforcement Learning: An Introduction - Sutton & Barto -]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[Sutton's tutorial]: https://www.youtube.com/watch?v=Fsh1qMTg1xI
[minimal-RL]: https://github.com/seungeunrho/minimalRL
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
[Sutton note]: https://mclearninglab.tistory.com/category/Books/Sutton%20%EB%85%B8%ED%8A%B8
[SanghyukChun's Blog]: http://sanghyukchun.github.io/76/
