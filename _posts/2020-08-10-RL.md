---
title: "RL: An Intoduction(kor)"
date: 2020-08-10 20:39:28 -0400
categories: RL
---

목차는 silver아재꺼 다시 참고하기...   
초반에 algorithm 분류를 먼저 제시하면 좋을 것 같기도 하고   
일상언어로 표현 가능하게끔 작성   
그림도 많이 넣고   
수식도   


### 짧은 시간동안 개략적인 내용파악을 돕기 위한 글. 자세한 내용은 References를 보고 시간을 투자하면 알 수 있다.

## What is Reinforcement Learning?
![screenshot](https://user-images.githubusercontent.com/67356449/89783369-c7414980-db51-11ea-8db5-317bafab812b.png)

_fig1. Agent는 환경과 상호작용 하며 누적보상을 최대화 하기 위한 '좋은 행동'을 선택하게 된다._   
누적 보상의 최대값을 얻기 위해 가까운 시점의 보상을 희생하기도 한다

## Key Concepts
- RL, Environment, Model, Action, State, Reward

- Markov Decision Process

- Bellman Equation

- Dynamic Programming
    - Policy Evaluation
    - Policy Improvement
    - Policy Iteration
    
- Monte-Carlo Methods

- Temporal-Difference Learning
    - Bootstrapping
    - SARSA
    - Q-learning
    - Deep Q-Network

- Policy Gradient
    - REINFORCE
    - Actor-Critic
    - A3C

***
### 1. RL, Environment, Model, Action, State, Reward, Policy
강화학습이라고 불리는 방법은 기계학습에 속하는 다른 어떤 방법보다 상호작용으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법이다.   
**에이전트**는 어떤 행동을 취할지에 대한 어떠한 지침도 받지 않고   
오로지 시행착오(trial and error)를 통해 최대의 보상을 가져다주는 행동을 찾아내야만 한다.   
강화학습 **에이전트**와 그를 둘러싼 **환경**은 time step의 흐름 속에서 서로 상호작용 한다.   
이 사이 접점을 어떻게 규정하는지에 따라 문제의 특징이 정의된다.   
**행동**은 에이전트가 선택한 것이고, **상태**는 그 선택을 하기 위한 근간이 되며, **보상**은 선택을 평가하는 기준이 된다.   
   
**정책**은 주어진 상태에서 가장 좋은 보상을 가져다 줄 것으로 예측되는 행동을 선택하도록 한다.

    
### 2. MDP

### 3. Bellman Equation

### 4. Dynamic Programming
DP라는 용어는 MDP 같은 환경 모델이 완벽하게 주어졌을 때 최적 정책(Optimal Policy)을 계산하기 위해 사용될 수 있는 일군의 알고리즘을 가리킨다.   
고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산이 필요하다는 점 때문에 강화학습에서 그 활용도가 제한되었지만, 이론적으로는 현재까지도 여전히 중요하다.   
일단 최적 가치 함수 v* 또는 q* 를 구하고 나면, 최적 정책은 쉽게 구할 수 있다.   
_- RL: An introduction (sutton & Barto) -_

### 4. Bellman Equation

### 5. Monte-Carlo Methods

### 6. Temporal-Difference Learning
Sarsa: TD Q + epsilon greedy policy improvement + Update every time-step   
Q-Learning: sample-based Q-value iteration   
Q-Learning Properties   
Amazing result: Q-learning converges to optimal policy -- even if you’re acting suboptimally!   
This is called off-policy learning
### RL algorithm 분류
On-policy: "Learn on the job." -> behaviour policy와 target policy가 같다.   
Off-policy: "Look over someone's shoulder." -> behaviour policy를 따르는 와중에 optimal value function을 계산하기 위해 target policy를 평가함.


## REINFORCE는 왜 되는가?
Monte-Carlo Policy Gradient
theta update 하는 것, Gradient Ascent = 등산하기   

## DQN은 무엇이고 왜 되는가?
Function Approximation을 Nerual Network이용한 것   
Replay Memory는 sample 사이 연관성을 해소시키기 위함.   
왜 해소해야 하냐면..?   

## References
more info: [OpenAI DRL-spinning-up], [lilianweng's-post], [David Silver's lecture] ([lecture notes]), [Sutton & Barto]

한국어 자료: [<RL with python&keras> gitbook] ([gitbook code])
(이 자료는 Silver's Lecture에 기반하며 Silver는 Sutton의 책을 기준으로 설명)

[OpenAI DRL-spinning-up]: https://spinningup.openai.com/en/latest/index.html
[lilianweng's-post]: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
[David Silver's lecture]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[lecture notes]: https://www.davidsilver.uk/teaching/
[Sutton & Barto]: http://incompleteideas.net/book/bookdraft2018mar21.pdf
[<RL with python&keras> gitbook]: https://dnddnjs.gitbook.io/rl/
[gitbook code]: https://github.com/rlcode/reinforcement-learning-kr
